(window.webpackJsonp_N_E=window.webpackJsonp_N_E||[]).push([[16],{Igyz:function(e,n,t){"use strict";n.a="struct Scene {\n  lightViewProjMatrix: mat4x4<f32>,\n  cameraViewProjMatrix: mat4x4<f32>,\n  lightPos: vec3<f32>,\n}\n\nstruct Model {\n  modelMatrix: mat4x4<f32>,\n}\n\n@group(0) @binding(0) var<uniform> scene : Scene;\n@group(1) @binding(0) var<uniform> model : Model;\n\nstruct VertexOutput {\n  @location(0) shadowPos: vec3<f32>,\n  @location(1) fragPos: vec3<f32>,\n  @location(2) fragNorm: vec3<f32>,\n\n  @builtin(position) Position: vec4<f32>,\n}\n\n@vertex\nfn main(\n  @location(0) position: vec3<f32>,\n  @location(1) normal: vec3<f32>\n) -> VertexOutput {\n  var output : VertexOutput;\n\n  // XY is in (-1, 1) space, Z is in (0, 1) space\n  let posFromLight : vec4<f32> = scene.lightViewProjMatrix * model.modelMatrix * vec4<f32>(position, 1.0);\n\n  // Convert XY to (0, 1)\n  // Y is flipped because texture coords are Y-down.\n  output.shadowPos = vec3<f32>(\n    posFromLight.xy * vec2<f32>(0.5, -0.5) + vec2<f32>(0.5, 0.5),\n    posFromLight.z\n  );\n\n  output.Position = scene.cameraViewProjMatrix * model.modelMatrix * vec4<f32>(position, 1.0);\n  output.fragPos = output.Position.xyz;\n  output.fragNorm = normal;\n  return output;\n}\n"},QqoD:function(e,n,t){"use strict";t.d(n,"a",(function(){return c}));var r=t("FvBH"),a=t.n(r);function o(e,n){(null==n||n>e.length)&&(n=e.length);for(var t=0,r=new Array(n);t<n;t++)r[t]=e[t];return r}function i(e,n){return function(e){if(Array.isArray(e))return e}(e)||function(e,n){if("undefined"!==typeof Symbol&&Symbol.iterator in Object(e)){var t=[],r=!0,a=!1,o=void 0;try{for(var i,s=e[Symbol.iterator]();!(r=(i=s.next()).done)&&(t.push(i.value),!n||t.length!==n);r=!0);}catch(u){a=!0,o=u}finally{try{r||null==s.return||s.return()}finally{if(a)throw o}}return t}}(e,n)||function(e,n){if(e){if("string"===typeof e)return o(e,n);var t=Object.prototype.toString.call(e).slice(8,-1);return"Object"===t&&e.constructor&&(t=e.constructor.name),"Map"===t||"Set"===t?Array.from(e):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?o(e,n):void 0}}(e,n)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}var s=t("IOcx");var u={xy:[0,1],xz:[0,2],yz:[1,2]};var c={positions:a.a.positions,triangles:a.a.cells,normals:[],uvs:[]};c.normals=function(e,n){var t=e.map((function(){return[0,0,0]}));return n.forEach((function(n){var r=i(n,3),a=r[0],o=r[1],u=r[2],c=e[a],f=e[o],d=e[u],l=s.b.subtract(s.b.create(),f,c),p=s.b.subtract(s.b.create(),d,c);s.b.normalize(l,l),s.b.normalize(p,p);var m=s.b.cross(s.b.create(),l,p);s.b.add(t[a],t[a],m),s.b.add(t[o],t[o],m),s.b.add(t[u],t[u],m)})),t.forEach((function(e){s.b.normalize(e,e)})),t}(c.positions,c.triangles),c.uvs=function(e){var n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"xy",t=u[n],r=e.map((function(){return[0,0]})),a=[1/0,1/0],o=[-1/0,-1/0];return e.forEach((function(e,n){r[n][0]=e[t[0]],r[n][1]=e[t[1]],a[0]=Math.min(e[t[0]],a[0]),a[1]=Math.min(e[t[1]],a[1]),o[0]=Math.max(e[t[0]],o[0]),o[1]=Math.max(e[t[1]],o[1])})),r.forEach((function(e){e[0]=(e[0]-a[0])/(o[0]-a[0]),e[1]=(e[1]-a[1])/(o[1]-a[1])})),r}(c.positions,"xy"),c.triangles.push([c.positions.length,c.positions.length+2,c.positions.length+1],[c.positions.length,c.positions.length+1,c.positions.length+3]),c.positions.push([-100,20,-100],[100,20,100],[-100,20,100],[100,20,-100]),c.normals.push([0,1,0],[0,1,0],[0,1,0],[0,1,0]),c.uvs.push([0,0],[1,1],[0,1],[1,0])},iI8x:function(e,n,t){"use strict";n.a="struct Scene {\n  lightViewProjMatrix: mat4x4<f32>,\n  cameraViewProjMatrix: mat4x4<f32>,\n  lightPos: vec3<f32>,\n}\n\nstruct Model {\n  modelMatrix: mat4x4<f32>,\n}\n\n@group(0) @binding(0) var<uniform> scene : Scene;\n@group(1) @binding(0) var<uniform> model : Model;\n\n@vertex\nfn main(\n  @location(0) position: vec3<f32>\n) -> @builtin(position) vec4<f32> {\n  return scene.lightViewProjMatrix * model.modelMatrix * vec4<f32>(position, 1.0);\n}\n"},mIq5:function(e,n,t){"use strict";t.r(n),function(e,r){var a=t("o0o1"),o=t.n(a),i=t("HaE+"),s=t("IOcx"),u=t("8i9l"),c=t("QqoD"),f=t("iI8x"),d=t("Igyz"),l=t("nc7u"),p=function(){var e=Object(i.a)(o.a.mark((function e(n){var t,r,a,i,u,p,m,h,g,v,x,b,w,P,y,S,M,B,E,G,T,U,V,R,A,L,D,F,O,C,_,N,j,I,z,q,Y,X,W,k,H,J,Q,Z,$,K,ee,ne;return o.a.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return ne=function(){if(i){var e=K();a.queue.writeBuffer(D,64,e.buffer,e.byteOffset,e.byteLength),A.colorAttachments[0].view=u.getCurrentTexture().createView();var n=a.createCommandEncoder(),t=n.beginRenderPass(ee);t.setPipeline(T),t.setBindGroup(0,F),t.setBindGroup(1,C),t.setVertexBuffer(0,g),t.setIndexBuffer(w,"uint16"),t.drawIndexed(b),t.end();var r=n.beginRenderPass(A);r.setPipeline(V),r.setBindGroup(0,O),r.setBindGroup(1,C),r.setVertexBuffer(0,g),r.setIndexBuffer(w,"uint16"),r.drawIndexed(b),r.end(),a.queue.submit([n.finish()]),requestAnimationFrame(ne)}},K=function(){var e=s.b.fromValues(0,50,-100),n=Math.PI*(Date.now()/2e3);s.b.rotateY(e,e,j,n);var t=s.a.create();return s.a.lookAt(t,e,j,N),s.a.multiply(k,I,t),k},t=n.canvasRef,e.next=5,navigator.gpu.requestAdapter();case 5:return r=e.sent,e.next=8,r.requestDevice();case 8:if(a=e.sent,i=t.current,null!==t.current){e.next=12;break}return e.abrupt("return");case 12:for(u=t.current.getContext("webgpu"),p=navigator.gpu.getPreferredCanvasFormat(),m=[i.width,i.height],h=m[0]/m[1],u.configure({device:a,format:p,alphaMode:"opaque"}),g=a.createBuffer({size:3*c.a.positions.length*2*Float32Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.VERTEX,mappedAtCreation:!0}),v=new Float32Array(g.getMappedRange()),x=0;x<c.a.positions.length;++x)v.set(c.a.positions[x],6*x),v.set(c.a.normals[x],6*x+3);for(g.unmap(),b=3*c.a.triangles.length,w=a.createBuffer({size:b*Uint16Array.BYTES_PER_ELEMENT,usage:GPUBufferUsage.INDEX,mappedAtCreation:!0}),P=new Uint16Array(w.getMappedRange()),y=0;y<c.a.triangles.length;++y)P.set(c.a.triangles[y],3*y);w.unmap(),S=a.createTexture({size:[1024,1024,1],usage:GPUTextureUsage.RENDER_ATTACHMENT|GPUTextureUsage.TEXTURE_BINDING,format:"depth32float"}),M=S.createView(),B=[{arrayStride:6*Float32Array.BYTES_PER_ELEMENT,attributes:[{shaderLocation:0,offset:0,format:"float32x3"},{shaderLocation:1,offset:3*Float32Array.BYTES_PER_ELEMENT,format:"float32x3"}]}],E={topology:"triangle-list",cullMode:"back"},G=a.createBindGroupLayout({entries:[{binding:0,visibility:GPUShaderStage.VERTEX,buffer:{type:"uniform"}}]}),T=a.createRenderPipeline({layout:a.createPipelineLayout({bindGroupLayouts:[G,G]}),vertex:{module:a.createShaderModule({code:f.a}),entryPoint:"main",buffers:B},depthStencil:{depthWriteEnabled:!0,depthCompare:"less",format:"depth32float"},primitive:E}),U=a.createBindGroupLayout({entries:[{binding:0,visibility:GPUShaderStage.VERTEX|GPUShaderStage.FRAGMENT,buffer:{type:"uniform"}},{binding:1,visibility:GPUShaderStage.VERTEX|GPUShaderStage.FRAGMENT,texture:{sampleType:"depth"}},{binding:2,visibility:GPUShaderStage.VERTEX|GPUShaderStage.FRAGMENT,sampler:{type:"comparison"}}]}),V=a.createRenderPipeline({layout:a.createPipelineLayout({bindGroupLayouts:[U,G]}),vertex:{module:a.createShaderModule({code:d.a}),entryPoint:"main",buffers:B},fragment:{module:a.createShaderModule({code:l.a}),entryPoint:"main",targets:[{format:p}]},depthStencil:{depthWriteEnabled:!0,depthCompare:"less",format:"depth24plus-stencil8"},primitive:E}),R=a.createTexture({size:m,format:"depth24plus-stencil8",usage:GPUTextureUsage.RENDER_ATTACHMENT}),A={colorAttachments:[{view:void 0,clearValue:{r:.5,g:.5,b:.5,a:1},loadOp:"clear",storeOp:"store"}],depthStencilAttachment:{view:R.createView(),depthClearValue:1,depthLoadOp:"clear",depthStoreOp:"store",stencilClearValue:0,stencilLoadOp:"clear",stencilStoreOp:"store"}},L=a.createBuffer({size:64,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),D=a.createBuffer({size:140,usage:GPUBufferUsage.UNIFORM|GPUBufferUsage.COPY_DST}),F=a.createBindGroup({layout:G,entries:[{binding:0,resource:{buffer:D}}]}),O=a.createBindGroup({layout:U,entries:[{binding:0,resource:{buffer:D}},{binding:1,resource:M},{binding:2,resource:a.createSampler({compare:"less"})}]}),C=a.createBindGroup({layout:G,entries:[{binding:0,resource:{buffer:L}}]}),_=s.b.fromValues(0,50,-100),N=s.b.fromValues(0,1,0),j=s.b.fromValues(0,0,0),I=s.a.create(),s.a.perspective(I,2*Math.PI/5,h,1,2e3),z=s.a.create(),s.a.lookAt(z,_,j,N),q=s.b.fromValues(50,100,-100),Y=s.a.create(),s.a.lookAt(Y,q,j,N),X=s.a.create(),-80,80,-80,80,-200,300,s.a.ortho(X,-80,80,-80,80,-200,300),W=s.a.create(),s.a.multiply(W,X,Y),k=s.a.create(),s.a.multiply(k,I,z),H=s.a.create(),s.a.translate(H,H,s.b.fromValues(0,-5,0)),s.a.translate(H,H,s.b.fromValues(0,-40,0)),J=W,a.queue.writeBuffer(D,0,J.buffer,J.byteOffset,J.byteLength),Q=k,a.queue.writeBuffer(D,64,Q.buffer,Q.byteOffset,Q.byteLength),Z=q,a.queue.writeBuffer(D,128,Z.buffer,Z.byteOffset,Z.byteLength),$=H,a.queue.writeBuffer(L,0,$.buffer,$.byteOffset,$.byteLength),ee={colorAttachments:[],depthStencilAttachment:{view:M,depthClearValue:1,depthLoadOp:"clear",depthStoreOp:"store"}},requestAnimationFrame(ne);case 76:case"end":return e.stop()}}),e)})));return function(n){return e.apply(this,arguments)}}();n.default=function(){return Object(u.a)({name:"Shadow Mapping",description:"This example shows how to sample from a depth texture to render shadows.",init:p,sources:[{name:e.substring(r.length+1),contents:"import { mat4, vec3 } from 'gl-matrix';\nimport { makeSample, SampleInit } from '../../components/SampleLayout';\n\nimport { mesh } from '../../meshes/stanfordDragon';\n\nimport vertexShadowWGSL from './vertexShadow.wgsl';\nimport vertexWGSL from './vertex.wgsl';\nimport fragmentWGSL from './fragment.wgsl';\n\nconst shadowDepthTextureSize = 1024;\n\nconst init: SampleInit = async ({ canvasRef }) => {\n  const adapter = await navigator.gpu.requestAdapter();\n  const device = await adapter.requestDevice();\n  const canvas = canvasRef.current;\n\n  if (canvasRef.current === null) return;\n  const context = canvasRef.current.getContext('webgpu') as GPUCanvasContext;\n\n  const presentationFormat = navigator.gpu.getPreferredCanvasFormat();\n  const presentationSize = [canvas.width, canvas.height];\n  const aspect = presentationSize[0] / presentationSize[1];\n\n  context.configure({\n    device,\n    format: presentationFormat,\n    alphaMode: 'opaque',\n  });\n\n  // Create the model vertex buffer.\n  const vertexBuffer = device.createBuffer({\n    size: mesh.positions.length * 3 * 2 * Float32Array.BYTES_PER_ELEMENT,\n    usage: GPUBufferUsage.VERTEX,\n    mappedAtCreation: true,\n  });\n  {\n    const mapping = new Float32Array(vertexBuffer.getMappedRange());\n    for (let i = 0; i < mesh.positions.length; ++i) {\n      mapping.set(mesh.positions[i], 6 * i);\n      mapping.set(mesh.normals[i], 6 * i + 3);\n    }\n    vertexBuffer.unmap();\n  }\n\n  // Create the model index buffer.\n  const indexCount = mesh.triangles.length * 3;\n  const indexBuffer = device.createBuffer({\n    size: indexCount * Uint16Array.BYTES_PER_ELEMENT,\n    usage: GPUBufferUsage.INDEX,\n    mappedAtCreation: true,\n  });\n  {\n    const mapping = new Uint16Array(indexBuffer.getMappedRange());\n    for (let i = 0; i < mesh.triangles.length; ++i) {\n      mapping.set(mesh.triangles[i], 3 * i);\n    }\n    indexBuffer.unmap();\n  }\n\n  // Create the depth texture for rendering/sampling the shadow map.\n  const shadowDepthTexture = device.createTexture({\n    size: [shadowDepthTextureSize, shadowDepthTextureSize, 1],\n    usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.TEXTURE_BINDING,\n    format: 'depth32float',\n  });\n  const shadowDepthTextureView = shadowDepthTexture.createView();\n\n  // Create some common descriptors used for both the shadow pipeline\n  // and the color rendering pipeline.\n  const vertexBuffers: Iterable<GPUVertexBufferLayout> = [\n    {\n      arrayStride: Float32Array.BYTES_PER_ELEMENT * 6,\n      attributes: [\n        {\n          // position\n          shaderLocation: 0,\n          offset: 0,\n          format: 'float32x3',\n        },\n        {\n          // normal\n          shaderLocation: 1,\n          offset: Float32Array.BYTES_PER_ELEMENT * 3,\n          format: 'float32x3',\n        },\n      ],\n    },\n  ];\n\n  const primitive: GPUPrimitiveState = {\n    topology: 'triangle-list',\n    cullMode: 'back',\n  };\n\n  const uniformBufferBindGroupLayout = device.createBindGroupLayout({\n    entries: [\n      {\n        binding: 0,\n        visibility: GPUShaderStage.VERTEX,\n        buffer: {\n          type: 'uniform',\n        },\n      },\n    ],\n  });\n\n  const shadowPipeline = device.createRenderPipeline({\n    layout: device.createPipelineLayout({\n      bindGroupLayouts: [\n        uniformBufferBindGroupLayout,\n        uniformBufferBindGroupLayout,\n      ],\n    }),\n    vertex: {\n      module: device.createShaderModule({\n        code: vertexShadowWGSL,\n      }),\n      entryPoint: 'main',\n      buffers: vertexBuffers,\n    },\n    depthStencil: {\n      depthWriteEnabled: true,\n      depthCompare: 'less',\n      format: 'depth32float',\n    },\n    primitive,\n  });\n\n  // Create a bind group layout which holds the scene uniforms and\n  // the texture+sampler for depth. We create it manually because the WebPU\n  // implementation doesn't infer this from the shader (yet).\n  const bglForRender = device.createBindGroupLayout({\n    entries: [\n      {\n        binding: 0,\n        visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT,\n        buffer: {\n          type: 'uniform',\n        },\n      },\n      {\n        binding: 1,\n        visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT,\n        texture: {\n          sampleType: 'depth',\n        },\n      },\n      {\n        binding: 2,\n        visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT,\n        sampler: {\n          type: 'comparison',\n        },\n      },\n    ],\n  });\n\n  const pipeline = device.createRenderPipeline({\n    layout: device.createPipelineLayout({\n      bindGroupLayouts: [bglForRender, uniformBufferBindGroupLayout],\n    }),\n    vertex: {\n      module: device.createShaderModule({\n        code: vertexWGSL,\n      }),\n      entryPoint: 'main',\n      buffers: vertexBuffers,\n    },\n    fragment: {\n      module: device.createShaderModule({\n        code: fragmentWGSL,\n      }),\n      entryPoint: 'main',\n      targets: [\n        {\n          format: presentationFormat,\n        },\n      ],\n    },\n    depthStencil: {\n      depthWriteEnabled: true,\n      depthCompare: 'less',\n      format: 'depth24plus-stencil8',\n    },\n    primitive,\n  });\n\n  const depthTexture = device.createTexture({\n    size: presentationSize,\n    format: 'depth24plus-stencil8',\n    usage: GPUTextureUsage.RENDER_ATTACHMENT,\n  });\n\n  const renderPassDescriptor: GPURenderPassDescriptor = {\n    colorAttachments: [\n      {\n        // view is acquired and set in render loop.\n        view: undefined,\n\n        clearValue: { r: 0.5, g: 0.5, b: 0.5, a: 1.0 },\n        loadOp: 'clear',\n        storeOp: 'store',\n      },\n    ],\n    depthStencilAttachment: {\n      view: depthTexture.createView(),\n\n      depthClearValue: 1.0,\n      depthLoadOp: 'clear',\n      depthStoreOp: 'store',\n      stencilClearValue: 0,\n      stencilLoadOp: 'clear',\n      stencilStoreOp: 'store',\n    },\n  };\n\n  const modelUniformBuffer = device.createBuffer({\n    size: 4 * 16, // 4x4 matrix\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n\n  const sceneUniformBuffer = device.createBuffer({\n    // Two 4x4 viewProj matrices,\n    // one for the camera and one for the light.\n    // Then a vec3 for the light position.\n    size: 2 * 4 * 16 + 3 * 4,\n    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,\n  });\n\n  const sceneBindGroupForShadow = device.createBindGroup({\n    layout: uniformBufferBindGroupLayout,\n    entries: [\n      {\n        binding: 0,\n        resource: {\n          buffer: sceneUniformBuffer,\n        },\n      },\n    ],\n  });\n\n  const sceneBindGroupForRender = device.createBindGroup({\n    layout: bglForRender,\n    entries: [\n      {\n        binding: 0,\n        resource: {\n          buffer: sceneUniformBuffer,\n        },\n      },\n      {\n        binding: 1,\n        resource: shadowDepthTextureView,\n      },\n      {\n        binding: 2,\n        resource: device.createSampler({\n          compare: 'less',\n        }),\n      },\n    ],\n  });\n\n  const modelBindGroup = device.createBindGroup({\n    layout: uniformBufferBindGroupLayout,\n    entries: [\n      {\n        binding: 0,\n        resource: {\n          buffer: modelUniformBuffer,\n        },\n      },\n    ],\n  });\n\n  const eyePosition = vec3.fromValues(0, 50, -100);\n  const upVector = vec3.fromValues(0, 1, 0);\n  const origin = vec3.fromValues(0, 0, 0);\n\n  const projectionMatrix = mat4.create();\n  mat4.perspective(projectionMatrix, (2 * Math.PI) / 5, aspect, 1, 2000.0);\n\n  const viewMatrix = mat4.create();\n  mat4.lookAt(viewMatrix, eyePosition, origin, upVector);\n\n  const lightPosition = vec3.fromValues(50, 100, -100);\n  const lightViewMatrix = mat4.create();\n  mat4.lookAt(lightViewMatrix, lightPosition, origin, upVector);\n\n  const lightProjectionMatrix = mat4.create();\n  {\n    const left = -80;\n    const right = 80;\n    const bottom = -80;\n    const top = 80;\n    const near = -200;\n    const far = 300;\n    mat4.ortho(lightProjectionMatrix, left, right, bottom, top, near, far);\n  }\n\n  const lightViewProjMatrix = mat4.create();\n  mat4.multiply(lightViewProjMatrix, lightProjectionMatrix, lightViewMatrix);\n\n  const viewProjMatrix = mat4.create();\n  mat4.multiply(viewProjMatrix, projectionMatrix, viewMatrix);\n\n  // Move the model so it's centered.\n  const modelMatrix = mat4.create();\n  mat4.translate(modelMatrix, modelMatrix, vec3.fromValues(0, -5, 0));\n  mat4.translate(modelMatrix, modelMatrix, vec3.fromValues(0, -40, 0));\n\n  // The camera/light aren't moving, so write them into buffers now.\n  {\n    const lightMatrixData = lightViewProjMatrix as Float32Array;\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      0,\n      lightMatrixData.buffer,\n      lightMatrixData.byteOffset,\n      lightMatrixData.byteLength\n    );\n\n    const cameraMatrixData = viewProjMatrix as Float32Array;\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      64,\n      cameraMatrixData.buffer,\n      cameraMatrixData.byteOffset,\n      cameraMatrixData.byteLength\n    );\n\n    const lightData = lightPosition as Float32Array;\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      128,\n      lightData.buffer,\n      lightData.byteOffset,\n      lightData.byteLength\n    );\n\n    const modelData = modelMatrix as Float32Array;\n    device.queue.writeBuffer(\n      modelUniformBuffer,\n      0,\n      modelData.buffer,\n      modelData.byteOffset,\n      modelData.byteLength\n    );\n  }\n\n  // Rotates the camera around the origin based on time.\n  function getCameraViewProjMatrix() {\n    const eyePosition = vec3.fromValues(0, 50, -100);\n\n    const rad = Math.PI * (Date.now() / 2000);\n    vec3.rotateY(eyePosition, eyePosition, origin, rad);\n\n    const viewMatrix = mat4.create();\n    mat4.lookAt(viewMatrix, eyePosition, origin, upVector);\n\n    mat4.multiply(viewProjMatrix, projectionMatrix, viewMatrix);\n    return viewProjMatrix as Float32Array;\n  }\n\n  const shadowPassDescriptor: GPURenderPassDescriptor = {\n    colorAttachments: [],\n    depthStencilAttachment: {\n      view: shadowDepthTextureView,\n      depthClearValue: 1.0,\n      depthLoadOp: 'clear',\n      depthStoreOp: 'store',\n    },\n  };\n\n  function frame() {\n    // Sample is no longer the active page.\n    if (!canvas) return;\n\n    const cameraViewProj = getCameraViewProjMatrix();\n    device.queue.writeBuffer(\n      sceneUniformBuffer,\n      64,\n      cameraViewProj.buffer,\n      cameraViewProj.byteOffset,\n      cameraViewProj.byteLength\n    );\n\n    renderPassDescriptor.colorAttachments[0].view = context\n      .getCurrentTexture()\n      .createView();\n\n    const commandEncoder = device.createCommandEncoder();\n    {\n      const shadowPass = commandEncoder.beginRenderPass(shadowPassDescriptor);\n      shadowPass.setPipeline(shadowPipeline);\n      shadowPass.setBindGroup(0, sceneBindGroupForShadow);\n      shadowPass.setBindGroup(1, modelBindGroup);\n      shadowPass.setVertexBuffer(0, vertexBuffer);\n      shadowPass.setIndexBuffer(indexBuffer, 'uint16');\n      shadowPass.drawIndexed(indexCount);\n\n      shadowPass.end();\n    }\n    {\n      const renderPass = commandEncoder.beginRenderPass(renderPassDescriptor);\n      renderPass.setPipeline(pipeline);\n      renderPass.setBindGroup(0, sceneBindGroupForRender);\n      renderPass.setBindGroup(1, modelBindGroup);\n      renderPass.setVertexBuffer(0, vertexBuffer);\n      renderPass.setIndexBuffer(indexBuffer, 'uint16');\n      renderPass.drawIndexed(indexCount);\n\n      renderPass.end();\n    }\n    device.queue.submit([commandEncoder.finish()]);\n    requestAnimationFrame(frame);\n  }\n  requestAnimationFrame(frame);\n};\n\nconst ShadowMapping: () => JSX.Element = () =>\n  makeSample({\n    name: 'Shadow Mapping',\n    description:\n      'This example shows how to sample from a depth texture to render shadows.',\n    init,\n    sources: [\n      {\n        name: __filename.substring(__dirname.length + 1),\n        contents: __SOURCE__,\n      },\n      {\n        name: './vertexShadow.wgsl',\n        contents: vertexShadowWGSL,\n        editable: true,\n      },\n      {\n        name: './vertex.wgsl',\n        contents: vertexWGSL,\n        editable: true,\n      },\n      {\n        name: './fragment.wgsl',\n        contents: fragmentWGSL,\n        editable: true,\n      },\n    ],\n    filename: __filename,\n  });\n\nexport default ShadowMapping;\n"},{name:"./vertexShadow.wgsl",contents:f.a,editable:!0},{name:"./vertex.wgsl",contents:d.a,editable:!0},{name:"./fragment.wgsl",contents:l.a,editable:!0}],filename:e})}}.call(this,"src/sample/shadowMapping/main.ts","src/sample/shadowMapping")},nc7u:function(e,n,t){"use strict";n.a="// TODO: Use pipeline constants\nconst shadowDepthTextureSize: f32 = 1024.0;\n\nstruct Scene {\n  lightViewProjMatrix : mat4x4<f32>,\n  cameraViewProjMatrix : mat4x4<f32>,\n  lightPos : vec3<f32>,\n}\n\n@group(0) @binding(0) var<uniform> scene : Scene;\n@group(0) @binding(1) var shadowMap: texture_depth_2d;\n@group(0) @binding(2) var shadowSampler: sampler_comparison;\n\nstruct FragmentInput {\n  @location(0) shadowPos : vec3<f32>,\n  @location(1) fragPos : vec3<f32>,\n  @location(2) fragNorm : vec3<f32>,\n}\n\nconst albedo : vec3<f32> = vec3<f32>(0.9, 0.9, 0.9);\nconst ambientFactor : f32 = 0.2;\n\n@fragment\nfn main(input : FragmentInput) -> @location(0) vec4<f32> {\n  // Percentage-closer filtering. Sample texels in the region\n  // to smooth the result.\n  var visibility : f32 = 0.0;\n  let oneOverShadowDepthTextureSize = 1.0 / shadowDepthTextureSize;\n  for (var y : i32 = -1 ; y <= 1 ; y = y + 1) {\n    for (var x : i32 = -1 ; x <= 1 ; x = x + 1) {\n      let offset : vec2<f32> = vec2<f32>(\n        f32(x) * oneOverShadowDepthTextureSize,\n        f32(y) * oneOverShadowDepthTextureSize\n      );\n\n      visibility += textureSampleCompare(\n        shadowMap, shadowSampler,\n        input.shadowPos.xy + offset, input.shadowPos.z - 0.007\n      );\n    }\n  }\n  visibility /= 9.0;\n\n  let lambertFactor : f32 = max(dot(normalize(scene.lightPos - input.fragPos), input.fragNorm), 0.0);\n  let lightingFactor : f32 = min(ambientFactor + visibility * lambertFactor, 1.0);\n  \n  return vec4<f32>(lightingFactor * albedo, 1.0);\n}\n"}}]);